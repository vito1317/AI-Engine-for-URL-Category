import os
import sqlite3
import requests
import time
import json
import re
from bs4 import BeautifulSoup
from collections import deque
from urllib.parse import urljoin, urlparse

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service as ChromeService
    from selenium.webdriver.chrome.options import Options as ChromeOptions
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

USE_LOCAL_AI = True 
LOCAL_AI_MODEL = 'deepseek-r1:32b' 
LOCAL_AI_URL = 'http://localhost:11434/api/generate' 

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL_NAME = 'gemini-2.5-pro'

if not USE_LOCAL_AI:
    try:
        import google.generativeai as genai
    except ImportError:
        print("éŒ¯èª¤ï¼šç•¶ä½¿ç”¨é›²ç«¯ AI æ™‚ï¼Œæœªå®‰è£ 'google-generativeai' å‡½å¼åº«ã€‚è«‹åŸ·è¡Œ 'pip install google-generativeai'")
        exit()

START_URLS = ["https://www.gamer.com.tw/", "https://www.dcard.tw/f", "https://www.ettoday.net/", "https://www.wikipedia.org/", "https://google.com.tw/", "https://www.yahoo.com/", "https://www.facebook.com/", "https://www.youtube.com/", "https://www.instagram.com/", "https://www.twitter.com/", "https://www.linkedin.com/", "https://www.reddit.com/", "https://www.quora.com/", "https://www.twitch.tv/", "https://www.netflix.com/", "https://www.amazon.com/", "https://www.ebay.com/", "https://www.alibaba.com/", "https://www.taobao.com/", "https://www.pchome.com.tw/", "https://www.ruten.com.tw/", "https://www.momoshop.com.tw/", "https://www.books.com.tw/", "https://www.cw.com.tw/", "https://www.chinatimes.com/", "https://www.ltn.com.tw/", "https://www.udn.com/", "https://www.bbc.com/", "https://www.cnn.com/", "https://www.nytimes.com/", "https://www.wsj.com/", "https://www.reuters.com/", "https://www.aljazeera.com/", "https://www.npr.org/", "https://www.bloomberg.com/", "https://www.forbes.com/", "https://www.theguardian.com/", "https://www.huffpost.com/", "https://www.vox.com/", "https://www.washingtonpost.com/", "https://www.vice.com/", "https://www.foxnews.com/", "https://www.infowars.com/", "https://github.com/"]
DB_NAME = "domain_classification.db"

MAX_DOMAINS_TO_CRAWL = 20

MAX_CLASSIFICATION_RETRIES = 3
RETRY_DELAY = 3

CLASSIFICATION_SCHEMA_JSON = """
{
  "classification_info": {
    "version": "2.0",
    "description": "ä¸€ä»½è©³ç´°çš„ç¶²ç«™åˆ†é¡æ³•ï¼Œé©ç”¨æ–¼å…§å®¹éæ¿¾ã€ç­–ç•¥åˆ†æèˆ‡æ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´ã€‚ç¸½è¨ˆåŒ…å« 11 å€‹ä¸»è¦é¡åˆ¥å’Œ 70 å€‹å­é¡åˆ¥ã€‚",
    "author": "Gemini"
  },
  "website_classifications": [
    {
      "code": "010",
      "category_zh": "ğŸ” æˆäººèˆ‡ä¸ç•¶å…§å®¹ (Adult & Inappropriate)",
      "description_zh": "é«˜é¢¨éšªé¡åˆ¥ï¼ŒåŒ…å«ä¸é©åˆæœªæˆå¹´äººæˆ–åœ¨ç‰¹å®šå ´åˆç€è¦½çš„å…§å®¹ï¼Œé€šå¸¸é è¨­å°é–ã€‚",
      "subcategories": [
        { "sub_code": "010-01", "name_zh": "æˆäººå…§å®¹/è‰²æƒ…", "description_zh": "æ˜ç¢ºçš„è‰²æƒ…åœ–ç‰‡ã€å½±ç‰‡æˆ–æ–‡å­—æè¿°ã€‚" },
        { "sub_code": "010-02", "name_zh": "è£¸éœ²/æ€§æš—ç¤º", "description_zh": "éè‰²æƒ…çš„è£¸éœ²ã€æ¸…æ¶¼æœè£æˆ–å¼·çƒˆæ€§æš—ç¤ºå…§å®¹ã€‚" },
        { "sub_code": "010-03", "name_zh": "æš´åŠ›/è¡€è…¥/ææ€–", "description_zh": "åŒ…å«çœŸå¯¦æˆ–è™›æ§‹çš„æš´åŠ›ã€è¡€è…¥ã€æ®˜é…·æˆ–ææ€–ç•«é¢ã€‚" },
        { "sub_code": "010-04", "name_zh": "æ¯’å“/é…’ç²¾/ç…™å“", "description_zh": "æå€¡ã€è²©å”®æˆ–è©³ç´°æè¿°éæ³•è—¥ç‰©ã€é…’ç²¾ã€ç…™è‰è£½å“çš„å…§å®¹ã€‚" },
        { "sub_code": "010-05", "name_zh": "æ­¦å™¨/è»ç«/çˆ†è£‚ç‰©", "description_zh": "é—œæ–¼æ­¦å™¨ã€è»ç«ã€ç‚¸è—¥çš„è£½é€ ã€æ”¹é€ èˆ‡äº¤æ˜“è³‡è¨Šã€‚" },
        { "sub_code": "010-06", "name_zh": "è³­åš/å½©ç¥¨", "description_zh": "æä¾›ç·šä¸Šè³­åšã€åšå¼ˆéŠæˆ²ã€å½©ç¥¨æŠ•æ³¨æˆ–ç›¸é—œè³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "010-07", "name_zh": "ä»‡æ¨è¨€è«–/æ¥µç«¯ä¸»ç¾©", "description_zh": "å®£æšé‡å°ç‰¹å®šæ—ç¾¤ã€å®—æ•™ã€æ€§åˆ¥çš„æ­§è¦–ã€ä»‡æ¨æˆ–æš´åŠ›è¨€è«–ã€‚" },
        { "sub_code": "010-08", "name_zh": "è‡ªæ®˜/è‡ªæ®º", "description_zh": "é¼“å‹µã€æè¿°æˆ–æä¾›è‡ªæ®˜èˆ‡è‡ªæ®ºæ–¹æ³•çš„å…§å®¹ã€‚" }
      ]
    },
    {
      "code": "020",
      "category_zh": "âš ï¸ ç¶²è·¯é¢¨éšªèˆ‡å®‰å…¨ (Web Risk & Security)",
      "description_zh": "å¯èƒ½å°ä½¿ç”¨è€…è¨­å‚™ã€å€‹äººè³‡è¨Šæˆ–è²¡ç”¢æ§‹æˆå¨è„…çš„ç¶²ç«™ï¼Œå»ºè­°å°é–æˆ–å¯©æŸ¥ã€‚",
      "subcategories": [
        { "sub_code": "020-01", "name_zh": "æƒ¡æ„ç¶²ç«™/ç—…æ¯’", "description_zh": "æ•£æ’­æƒ¡æ„è»Ÿé«”ã€ç—…æ¯’ã€å‹’ç´¢è»Ÿé«”æˆ–æœ¨é¦¬çš„ç¶²ç«™ã€‚" },
        { "sub_code": "020-02", "name_zh": "è©é¨™/ç¶²è·¯é‡£é­š", "description_zh": "ä¼åœ–é¨™å–ä½¿ç”¨è€…å€‹è³‡ã€å¸³è™Ÿå¯†ç¢¼æˆ–é‡‘éŒ¢çš„æ¬ºè©ç¶²ç«™ã€‚" },
        { "sub_code": "020-03", "name_zh": "åŒ¿å/è¦é¿å·¥å…·", "description_zh": "æä¾› VPNã€Proxyã€Tor ç­‰ç”¨æ–¼éš±è—èº«ä»½æˆ–è¦é¿ç¶²è·¯å¯©æŸ¥çš„å·¥å…·ã€‚" },
        { "sub_code": "020-04", "name_zh": "P2P/éæ³•ä¸‹è¼‰", "description_zh": "æä¾› BT ç¨®å­ã€æª”æ¡ˆå…±äº«æˆ–ç›œç‰ˆè»Ÿé«”ã€å½±éŸ³å…§å®¹ä¸‹è¼‰çš„ç¶²ç«™ã€‚" },
        { "sub_code": "020-05", "name_zh": "å»£å‘Š/è¿½è¹¤å™¨", "description_zh": "ä¸»è¦ç›®çš„ç‚ºæŠ•æ”¾å¤§é‡å»£å‘Šæˆ–é€²è¡Œè·¨ç¶²ç«™ä½¿ç”¨è€…è¡Œç‚ºè¿½è¹¤ã€‚" },
        { "sub_code": "020-06", "name_zh": "åƒåœ¾éƒµä»¶/å¯ç–‘ç¶²ç«™", "description_zh": "è¢«æ¨™è¨˜ç‚ºåƒåœ¾éƒµä»¶ä¾†æºæˆ–è¡Œç‚ºå¯ç–‘çš„ç¶²ç«™ã€‚" }
      ]
    },
    {
      "code": "030",
      "category_zh": "ğŸŒ ç¤¾ç¾¤èˆ‡é€šè¨Š (Social & Communication)",
      "description_zh": "ä½¿ç”¨è€…ä¹‹é–“é€²è¡Œäº’å‹•ã€æºé€šèˆ‡å…§å®¹åˆ†äº«çš„å¹³å°ï¼Œå¯è¦–å¹´é½¡èˆ‡æƒ…å¢ƒå½ˆæ€§æ§ç®¡ã€‚",
      "subcategories": [
        { "sub_code": "030-01", "name_zh": "ç¶œåˆç¤¾ç¾¤åª’é«”", "description_zh": "å¦‚ Facebookã€Instagram ç­‰å¤šåŠŸèƒ½ç¤¾äº¤å¹³å°ã€‚" },
        { "sub_code": "030-02", "name_zh": "å³æ™‚é€šè¨Š", "description_zh": "å¦‚ LINEã€Discordã€Telegram ç­‰å³æ™‚è¨Šæ¯æºé€šå·¥å…·ã€‚" },
        { "sub_code": "030-03", "name_zh": "è«–å£‡/ä½¿ç”¨è€…ç”Ÿæˆå…§å®¹", "description_zh": "å¦‚ Redditã€PTT ç­‰ä»¥ç‰¹å®šä¸»é¡Œç‚ºæ ¸å¿ƒçš„è¨è«–å€ã€‚" },
        { "sub_code": "030-04", "name_zh": "éƒ¨è½æ ¼å¹³å°", "description_zh": "æä¾›å€‹äººæˆ–åœ˜é«”ç™¼è¡¨æ–‡ç« ã€è§€é»çš„å¹³å°ã€‚" },
        { "sub_code": "030-05", "name_zh": "ç·šä¸Šäº¤å‹", "description_zh": "ä»¥å°‹æ‰¾æˆ€æ„›æˆ–ç¤¾äº¤é—œä¿‚ç‚ºç›®çš„çš„ç´„æœƒç¶²ç«™æˆ– Appã€‚" }
      ]
    },
    {
      "code": "040",
      "category_zh": "ğŸ¬ å¨›æ¨‚èˆ‡åª’é«” (Entertainment & Media)",
      "description_zh": "æä¾›æ¶ˆé£ã€ä¼‘é–’èˆ‡æ„Ÿå®˜é«”é©—ç‚ºä¸»çš„å…§å®¹ã€‚",
      "subcategories": [
        { "sub_code": "040-01", "name_zh": "å½±éŸ³ä¸²æµ", "description_zh": "å¦‚ YouTubeã€Netflixã€Twitch ç­‰å½±éŸ³å¹³å°ã€‚" },
        { "sub_code": "040-02", "name_zh": "éŸ³æ¨‚/å»£æ’­", "description_zh": "å¦‚ Spotifyã€Apple Music ç­‰éŸ³æ¨‚ä¸²æµæˆ–ç¶²è·¯å»£æ’­ã€‚" },
        { "sub_code": "040-03", "name_zh": "ç·šä¸ŠéŠæˆ²", "description_zh": "æä¾›ç¶²é éŠæˆ²ã€å®¢æˆ¶ç«¯éŠæˆ²ä¸‹è¼‰æˆ–éŠæˆ²è³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "040-04", "name_zh": "é›»å½±/é›»è¦–", "description_zh": "æä¾›é›»å½±ã€é›»è¦–åŠ‡è³‡è¨Šã€è©•è«–æˆ–æ™‚åˆ»è¡¨çš„ç¶²ç«™ã€‚" },
        { "sub_code": "040-05", "name_zh": "å¹½é»˜/è¿·å› ", "description_zh": "ä»¥ç¬‘è©±ã€è¶£åœ–ã€è¿·å› ç­‰è¼•é¬†å…§å®¹ç‚ºä¸»çš„ç¶²ç«™ã€‚" },
        { "sub_code": "040-06", "name_zh": "å‹•æ¼«", "description_zh": "æä¾›å‹•ç•«ã€æ¼«ç•«ç·šä¸Šè§€çœ‹æˆ–ç›¸é—œè³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "040-07", "name_zh": "åäººå…«å¦", "description_zh": "å°ˆæ³¨æ–¼å ±å°è—äººã€åäººå‹•æ…‹èˆ‡èŠ±é‚Šæ–°èçš„ç¶²ç«™ã€‚" }
      ]
    },
    {
      "code": "050",
      "category_zh": "ğŸ›ï¸ å•†æ¥­èˆ‡è³¼ç‰© (Commerce & Shopping)",
      "description_zh": "æ¶‰åŠå•†å“ã€æœå‹™äº¤æ˜“æˆ–å•†æ¥­æ´»å‹•çš„ç¶²ç«™ã€‚",
      "subcategories": [
        { "sub_code": "050-01", "name_zh": "ç·šä¸Šè³¼ç‰©/é›»å•†", "description_zh": "å¦‚ Amazonã€PChome ç­‰ç¶œåˆæˆ–å‚ç›´å‹é›»å­å•†å‹™å¹³å°ã€‚" },
        { "sub_code": "050-02", "name_zh": "æ‹è³£/äºŒæ‰‹äº¤æ˜“", "description_zh": "å¦‚ eBayã€æ—‹è½‰æ‹è³£ç­‰ C2C æˆ– B2C æ‹è³£ç¶²ç«™ã€‚" },
        { "sub_code": "050-03", "name_zh": "æ¯”åƒ¹ç¶²ç«™", "description_zh": "æä¾›å•†å“æˆ–æœå‹™åƒ¹æ ¼æ¯”è¼ƒè³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "050-04", "name_zh": "åœ˜è³¼", "description_zh": "æä¾›é›†é«”è³¼è²·ä»¥ç²å¾—å„ªæƒ çš„å¹³å°ã€‚" },
        { "sub_code": "050-05", "name_zh": "åˆ†é¡å»£å‘Š", "description_zh": "æä¾›åœ°å€æ€§çš„å•†å“ã€æœå‹™ã€æ±‚è·ç­‰åˆ†é¡è³‡è¨ŠåˆŠç™»ã€‚" }
      ]
    },
    {
      "code": "060",
      "category_zh": "ğŸ’° é‡‘èèˆ‡å•†æ¥­æœå‹™ (Finance & Business Services)",
      "description_zh": "æä¾›é‡‘èã€æŠ•è³‡ã€æ³•å¾‹ã€å•†æ¥­ç›¸é—œæœå‹™çš„ç¶²ç«™ã€‚",
      "subcategories": [
        { "sub_code": "060-01", "name_zh": "é‡‘è/éŠ€è¡Œ", "description_zh": "æä¾›ç¶²è·¯éŠ€è¡Œã€ä¿éšªã€è²¸æ¬¾ç­‰æœå‹™çš„é‡‘èæ©Ÿæ§‹ç¶²ç«™ã€‚" },
        { "sub_code": "060-02", "name_zh": "æŠ•è³‡/è‚¡ç¥¨", "description_zh": "æä¾›è‚¡ç¥¨ã€åŸºé‡‘ã€å¤–åŒ¯ç­‰å¸‚å ´è³‡è¨Šèˆ‡äº¤æ˜“å¹³å°ã€‚" },
        { "sub_code": "060-03", "name_zh": "åŠ å¯†è²¨å¹£", "description_zh": "æä¾›åŠ å¯†è²¨å¹£äº¤æ˜“ã€è³‡è¨Šæˆ–ç›¸é—œæœå‹™çš„å¹³å°ã€‚" },
        { "sub_code": "060-04", "name_zh": "ä¼æ¥­ç¶²ç«™", "description_zh": "éè³¼ç‰©å°å‘çš„ä¼æ¥­å½¢è±¡èˆ‡è³‡è¨Šç¶²ç«™ã€‚" },
        { "sub_code": "060-05", "name_zh": "æ±‚è·/äººåŠ›è³‡æº", "description_zh": "æä¾›è·ç¼ºæœå°‹ã€å±¥æ­·åˆŠç™»èˆ‡æ‹›å‹Ÿæœå‹™çš„ç¶²ç«™ã€‚" },
        { "sub_code": "060-06", "name_zh": "æˆ¿åœ°ç”¢", "description_zh": "æä¾›æˆ¿å±‹è²·è³£ã€ç§Ÿè³ƒèˆ‡ç›¸é—œè³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "060-07", "name_zh": "æ³•å¾‹æœå‹™", "description_zh": "æä¾›æ³•å¾‹è«®è©¢ã€äº‹å‹™æ‰€è³‡è¨Šæˆ–æ³•è¦æŸ¥è©¢çš„ç¶²ç«™ã€‚" }
      ]
    },
    {
      "code": "070",
      "category_zh": "ğŸ“– æ•™è‚²èˆ‡ç›Šæ™º (Education & Reference)",
      "description_zh": "æä¾›çŸ¥è­˜ç²å–ã€æŠ€èƒ½å­¸ç¿’èˆ‡åƒè€ƒè³‡è¨Šçš„ç¶²ç«™ï¼Œé€šå¸¸é è¨­å…è¨±ã€‚",
      "subcategories": [
        { "sub_code": "070-01", "name_zh": "æ•™è‚²è³‡æº/ç·šä¸Šå­¸ç¿’", "description_zh": "å¦‚ Courseraã€Khan Academy ç­‰ç·šä¸Šèª²ç¨‹æˆ–æ•™å­¸è³‡æºç¶²ç«™ã€‚" },
        { "sub_code": "070-02", "name_zh": "å­¸æ ¡/æ•™è‚²æ©Ÿæ§‹", "description_zh": "å„ç´šå­¸æ ¡ã€å¤§å­¸ã€ç ”ç©¶æ©Ÿæ§‹çš„å®˜æ–¹ç¶²ç«™ã€‚" },
        { "sub_code": "070-03", "name_zh": "åœ–æ›¸/å­—å…¸/ç™¾ç§‘", "description_zh": "æä¾›ç·šä¸Šé–±è®€ã€å­—å…¸æŸ¥è©¢ã€ç™¾ç§‘å…¨æ›¸ç­‰æœå‹™çš„ç¶²ç«™ã€‚" },
        { "sub_code": "070-04", "name_zh": "èªè¨€å­¸ç¿’", "description_zh": "å°ˆé–€ç”¨æ–¼å­¸ç¿’å¤–èªçš„ç¶²ç«™æˆ–å·¥å…·ã€‚" },
        { "sub_code": "070-05", "name_zh": "æ­·å²/äººæ–‡", "description_zh": "æä¾›æ­·å²ã€åœ°ç†ã€è—è¡“ç­‰äººæ–‡ç§‘å­¸çŸ¥è­˜çš„ç¶²ç«™ã€‚" },
        { "sub_code": "070-06", "name_zh": "ç§‘å­¸/è‡ªç„¶", "description_zh": "æä¾›ç‰©ç†ã€åŒ–å­¸ã€ç”Ÿç‰©ç­‰è‡ªç„¶ç§‘å­¸çŸ¥è­˜çš„ç¶²ç«™ã€‚" }
      ]
    },
    {
      "code": "080",
      "category_zh": "ğŸ“° è³‡è¨Šèˆ‡æ–°è (Information & News)",
      "description_zh": "ä»¥å‚³éæ™‚äº‹ã€è³‡è¨Šã€è§€é»ç‚ºä¸»è¦ç›®çš„çš„ç¶²ç«™ã€‚",
      "subcategories": [
        { "sub_code": "080-01", "name_zh": "æ–°è/åª’é«”", "description_zh": "åœ‹å…§å¤–ç¶œåˆæ€§ã€åœ°æ–¹æ€§æˆ–å°ˆæ¥­é ˜åŸŸçš„æ–°èåª’é«”ç¶²ç«™ã€‚" },
        { "sub_code": "080-02", "name_zh": "å¤©æ°£", "description_zh": "æä¾›å¤©æ°£é å ±èˆ‡æ°£è±¡è³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "080-03", "name_zh": "åœ°åœ–/å°èˆª", "description_zh": "æä¾›åœ°åœ–æŸ¥è©¢ã€è·¯ç·šè¦åŠƒèˆ‡å®šä½æœå‹™çš„ç¶²ç«™ã€‚" },
        { "sub_code": "080-04", "name_zh": "æ”¿åºœæ©Ÿæ§‹", "description_zh": "å„åœ‹ä¸­å¤®èˆ‡åœ°æ–¹æ”¿åºœçš„å®˜æ–¹å…¥å£ç¶²ç«™ã€‚" },
        { "sub_code": "080-05", "name_zh": "éç‡Ÿåˆ©çµ„ç¹”", "description_zh": "éæ”¿åºœã€éç‡Ÿåˆ©çš„æ…ˆå–„ã€ç’°ä¿ã€äººæ¬Šç­‰çµ„ç¹”ç¶²ç«™ã€‚" }
      ]
    },
    {
      "code": "090",
      "category_zh": "â¤ï¸ ç”Ÿæ´»èˆ‡å¥åº· (Lifestyle & Health)",
      "description_zh": "èˆ‡æ—¥å¸¸ç”Ÿæ´»ã€å€‹äººå¥åº·ã€èˆˆè¶£å—œå¥½ç›¸é—œçš„ç¶²ç«™ã€‚",
      "subcategories": [
        { "sub_code": "090-01", "name_zh": "å¥åº·/é†«ç™‚", "description_zh": "æä¾›å¥åº·è³‡è¨Šã€é†«ç™‚çŸ¥è­˜ã€é†«é™¢è¨ºæ‰€æŸ¥è©¢çš„ç¶²ç«™ã€‚" },
        { "sub_code": "090-02", "name_zh": "é¤é£²/é£Ÿè­œ", "description_zh": "æä¾›é¤å»³è©•è«–ã€é£Ÿè­œåˆ†äº«ã€ç¾é£Ÿè³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "090-03", "name_zh": "æ—…éŠ/è¨‚ç¥¨", "description_zh": "æä¾›æ—…éŠè³‡è¨Šã€è¡Œç¨‹è¦åŠƒã€æ©Ÿç¥¨é£¯åº—é è¨‚çš„ç¶²ç«™ã€‚" },
        { "sub_code": "090-04", "name_zh": "æ™‚å°š/ç¾å®¹", "description_zh": "æä¾›æµè¡Œç©¿æ­ã€ç¾å¦ä¿é¤Šè³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "090-05", "name_zh": "æ±½è»Š", "description_zh": "æä¾›æ±½è»Šè³‡è¨Šã€è©•æ¸¬ã€è²·è³£èˆ‡ç¤¾ç¾¤çš„ç¶²ç«™ã€‚" },
        { "sub_code": "090-06", "name_zh": "é‹å‹•/å¥èº«", "description_zh": "æä¾›é‹å‹•æ•™å­¸ã€è³½äº‹å ±å°ã€å¥èº«è³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "090-07", "name_zh": "å¯µç‰©", "description_zh": "æä¾›å¯µç‰©é£¼é¤Šã€é†«ç™‚ã€ç¤¾ç¾¤ç­‰è³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "090-08", "name_zh": "å®—æ•™", "description_zh": "æä¾›ç‰¹å®šå®—æ•™æ•™ç¾©ã€æ´»å‹•èˆ‡è³‡è¨Šçš„ç¶²ç«™ã€‚" }
      ]
    },
    {
      "code": "100",
      "category_zh": "ğŸ› ï¸ æŠ€è¡“èˆ‡å·¥å…· (Technology & Tools)",
      "description_zh": "æä¾›è»Ÿé«”ã€ç¡¬é«”ã€ç¶²è·¯æŠ€è¡“è³‡è¨Šæˆ–ç·šä¸Šå¯¦ç”¨å·¥å…·çš„ç¶²ç«™ã€‚",
      "subcategories": [
        { "sub_code": "100-01", "name_zh": "æœå°‹å¼•æ“", "description_zh": "å¦‚ Googleã€Bing ç­‰ç”¨æ–¼æœå°‹ç¶²éš›ç¶²è·¯è³‡è¨Šçš„ç¶²ç«™ã€‚" },
        { "sub_code": "100-02", "name_zh": "ç§‘æŠ€æ–°è/è©•è«–", "description_zh": "å°ˆæ³¨æ–¼å ±å° 3Cã€è»Ÿé«”ã€ç¶²è·¯èˆ‡ç§‘æŠ€ç”¢æ¥­å‹•æ…‹çš„åª’é«”ã€‚" },
        { "sub_code": "100-03", "name_zh": "ç¶²é éƒµä»¶", "description_zh": "æä¾›ç¶²é ç‰ˆé›»å­éƒµä»¶æ”¶ç™¼æœå‹™çš„ç¶²ç«™ã€‚" },
        { "sub_code": "100-04", "name_zh": "é›²ç«¯å„²å­˜/æª”æ¡ˆåˆ†äº«", "description_zh": "æä¾›ç·šä¸Šæª”æ¡ˆå„²å­˜ã€åŒæ­¥èˆ‡åˆ†äº«æœå‹™çš„ç¶²ç«™ã€‚" },
        { "sub_code": "100-05", "name_zh": "è»Ÿé«”ä¸‹è¼‰", "description_zh": "æä¾›è»Ÿé«”ã€æ‡‰ç”¨ç¨‹å¼ä¸‹è¼‰çš„è³‡æºåº«æˆ–å¸‚é›†ã€‚" },
        { "sub_code": "100-06", "name_zh": "ç·šä¸Šå·¥å…·", "description_zh": "æä¾›æª”æ¡ˆè½‰æ›ã€åœ–ç‰‡ç·¨è¼¯ã€ç·šä¸Šç¿»è­¯ç­‰å¯¦ç”¨åŠŸèƒ½çš„ç¶²ç«™ã€‚" },
        { "sub_code": "100-07", "name_zh": "é–‹ç™¼è€…è³‡æº", "description_zh": "æä¾›ç¨‹å¼ç¢¼è¨—ç®¡ã€API æ–‡ä»¶ã€æŠ€è¡“å•ç­”ç­‰é–‹ç™¼è€…æ‰€éœ€è³‡æºã€‚" }
      ]
    },
    {
      "code": "999",
      "category_zh": "âš™ï¸ ç³»çµ±åˆ†é¡ (System Categories)",
      "description_zh": "ç”¨æ–¼åˆ†é¡æµç¨‹ç®¡ç†çš„ç‰¹æ®Šé¡åˆ¥ï¼Œéå…§å®¹å°å‘ã€‚",
      "subcategories": [
        { "sub_code": "999-01", "name_zh": "å¾…åˆ†é¡", "description_zh": "å·²ç™¼ç¾ä½†å°šæœªé€²è¡Œäººå·¥æˆ–è‡ªå‹•åˆ†é¡çš„ç¶²ç«™ã€‚" },
        { "sub_code": "999-02", "name_zh": "ç„¡æ³•è¨ªå•/éŒ¯èª¤", "description_zh": "ç„¡æ³•æ­£å¸¸é€£ç·šã€é¡¯ç¤ºéŒ¯èª¤è¨Šæ¯æˆ–å·²å¤±æ•ˆçš„ç¶²ç«™ã€‚" },
        { "sub_code": "999-03", "name_zh": "åœæ³Šç¶²åŸŸ", "description_zh": "å·²è¨»å†Šä½†æ²’æœ‰å¯¦éš›å…§å®¹ï¼Œåƒ…é¡¯ç¤ºå»£å‘Šæˆ–ã€Œå»ºç½®ä¸­ã€çš„ç¶²åŸŸã€‚" },
        { "sub_code": "999-04", "name_zh": "ç§äºº IP", "description_zh": "æŒ‡å‘å…§éƒ¨ç¶²è·¯æˆ–ä¿ç•™ IP ä½å€çš„ç¶²ç«™ã€‚" },
        { "sub_code": "999-99", "name_zh": "æœªçŸ¥", "description_zh": "ç¶“éåˆ†é¡æµç¨‹å¾Œï¼Œä»ç„¡æ³•æ­¸é¡çš„ç¶²ç«™ã€‚" }
      ]
    }
  ]
}
"""

CLASSIFICATION_SCHEMA = json.loads(CLASSIFICATION_SCHEMA_JSON)

def build_code_maps(schema):
    main_category_map, subcategory_map = {}, {}
    for category in schema["website_classifications"]:
        main_category_map[category["code"]] = category["category_zh"]
        for sub in category["subcategories"]:
            subcategory_map[sub["sub_code"]] = sub["name_zh"]
    return main_category_map, subcategory_map

MAIN_CATEGORY_MAP, SUBCATEGORY_MAP = build_code_maps(CLASSIFICATION_SCHEMA)


class DatabaseManager:
    """è² è²¬è™•ç†æ‰€æœ‰èˆ‡ SQLite è³‡æ–™åº«ç›¸é—œçš„æ“ä½œ"""
    def __init__(self, db_name):
        self.conn = sqlite3.connect(db_name)
        self.cursor = self.conn.cursor()
        print(f"è³‡æ–™åº« '{db_name}' é€£ç·šæˆåŠŸã€‚")

    def setup_tables(self):
        """å»ºç«‹æ‰€æœ‰éœ€è¦çš„è³‡æ–™è¡¨"""
        try:
            self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS classified_domains (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                domain TEXT NOT NULL UNIQUE,
                main_category_code TEXT,
                main_category_name TEXT,
                subcategory_code TEXT,
                subcategory_name TEXT,
                summary TEXT,
                source_url TEXT,
                crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """)
            self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS crawl_queue (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT NOT NULL UNIQUE
            )
            """)
            self.conn.commit()
            print("è³‡æ–™è¡¨ 'classified_domains' å’Œ 'crawl_queue' å·²å»ºç«‹æˆ–å·²å­˜åœ¨ã€‚")
        except sqlite3.Error as e:
            print(f"å»ºç«‹è³‡æ–™è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def add_domain_classification(self, domain, main_cat_code, main_cat_name, sub_cat_code, sub_cat_name, summary, source_url):
        try:
            self.cursor.execute(
                """INSERT OR IGNORE INTO classified_domains 
                   (domain, main_category_code, main_category_name, subcategory_code, subcategory_name, summary, source_url) 
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (domain, main_cat_code, main_cat_name, sub_cat_code, sub_cat_name, summary, source_url)
            )
            self.conn.commit()
            print(f"æˆåŠŸå°‡åˆ†é¡ç´€éŒ„æ–°å¢è‡³è³‡æ–™åº«: {domain}")
        except sqlite3.Error as e:
            print(f"æ–°å¢è³‡æ–™è‡³è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def add_to_queue(self, url):
        """å°‡å–®ä¸€ URL åŠ å…¥å¾…çˆ¬å–ä½‡åˆ—è³‡æ–™è¡¨"""
        try:
            self.cursor.execute("INSERT OR IGNORE INTO crawl_queue (url) VALUES (?)", (url,))
            self.conn.commit()
        except sqlite3.Error as e:
            print(f"å°‡ URL åŠ å…¥ä½‡åˆ—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def remove_from_queue(self, url):
        """å¾å¾…çˆ¬å–ä½‡åˆ—è³‡æ–™è¡¨ä¸­ç§»é™¤å–®ä¸€ URL"""
        try:
            self.cursor.execute("DELETE FROM crawl_queue WHERE url = ?", (url,))
            self.conn.commit()
        except sqlite3.Error as e:
            print(f"å¾ä½‡åˆ—ç§»é™¤ URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def load_queue(self):
        """å¾è³‡æ–™åº«è¼‰å…¥æ•´å€‹å¾…çˆ¬å–ä½‡åˆ—"""
        try:
            self.cursor.execute("SELECT url FROM crawl_queue")
            return [row[0] for row in self.cursor.fetchall()]
        except sqlite3.Error as e:
            print(f"è¼‰å…¥ä½‡åˆ—æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []

    def domain_exists(self, domain):
        self.cursor.execute("SELECT id FROM classified_domains WHERE domain = ?", (domain,))
        return self.cursor.fetchone() is not None

    def close(self):
        self.conn.close()
        print("è³‡æ–™åº«é€£ç·šå·²é—œé–‰ã€‚")


def get_summary_prompt(text_content):
    """ç”¢ç”Ÿç”¨æ–¼ç¬¬ä¸€éšæ®µã€Œæ‘˜è¦ã€çš„æç¤º"""
    return f"""Analyze the following website text content and provide a single, concise, one-sentence summary in Traditional Chinese that describes the website's primary purpose.

**SPECIAL RULE:** If the content appears to be a security check, firewall, or block page (e.g., from Cloudflare), your summary MUST be "é€™æ˜¯ä¸€å€‹é˜²ç«ç‰†æˆ–å®‰å…¨æª¢æŸ¥é é¢ã€‚".

**Website Text Content:**
---
{text_content}
---

Your response MUST be only the one-sentence summary and nothing else.
"""

def get_classification_from_summary_prompt(schema_str, url, summary):
    """ç”¢ç”Ÿç”¨æ–¼ç¬¬äºŒéšæ®µã€ŒåŸºæ–¼æ‘˜è¦å’ŒURLåˆ†é¡ã€çš„æç¤º"""
    return f"""You are a JSON-generating robot. Your task is to classify the provided website based on its URL and summary.

**Classification Schema:**
---
{schema_str}
---

**Information to Classify:**
- **URL:** `{url}`
- **Summary:** "{summary}"

**INSTRUCTIONS:**
1.  Analyze both the URL and the Summary to understand the website's true identity and purpose. The URL provides crucial context.
2.  Based on your combined analysis, choose the most accurate `main_category_code` and `subcategory_code` from the schema.
3.  Construct a JSON object with your results.

**SPECIAL RULE:** If the summary is "é€™æ˜¯ä¸€å€‹é˜²ç«ç‰†æˆ–å®‰å…¨æª¢æŸ¥é é¢ã€‚", you **MUST** classify it with `main_category_code: "999"` and `subcategory_code: "999-02"`.

**OUTPUT RULES:**
- Your response **MUST ONLY** be a single, valid JSON object.
- The JSON **MUST** contain two keys: `main_category_code` and `subcategory_code`.

Now, classify the website and provide ONLY the JSON object.
"""


class AIClassifier:
    """åˆ†é¡å™¨çš„æŠ½è±¡åŸºåº•é¡åˆ¥"""
    def classify_from_content(self, text_content, url):
        raise NotImplementedError

class LocalOllamaClassifier(AIClassifier):
    """ä½¿ç”¨æœ¬åœ°é‹è¡Œçš„ Ollama æœå‹™é€²è¡Œå…©éšæ®µåˆ†é¡"""
    def __init__(self, model, api_url, schema_json):
        self.model = model
        self.api_url = api_url
        self.schema_json_str = schema_json
        print(f"æœ¬åœ° Ollama åˆ†é¡å™¨å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡å‹: {self.model}")

    def _call_ollama(self, prompt, url_for_log, expect_json=False):
        payload = {"model": self.model, "prompt": prompt, "stream": False}
        if expect_json:
            payload["format"] = "json"
        
        try:
            print(f"æ­£åœ¨å‘æœ¬åœ° Ollama API è«‹æ±‚ ({'JSON' if expect_json else 'Text'}): {url_for_log}")
            response = requests.post(self.api_url, json=payload, timeout=180)
            response.raise_for_status()
            response_data = response.json()
            raw_response_str = response_data.get('response', '')
            print(f"DEBUG: Ollama åŸå§‹å›è¦†: {raw_response_str}")
            if not raw_response_str: return None
            
            cleaned_str = re.sub(r'<think>.*?</think>', '', raw_response_str, flags=re.DOTALL).strip()
            
            if expect_json:
                json_str = cleaned_str.lstrip('```json').rstrip('```').strip()
                return json.loads(json_str)
            else:
                return cleaned_str
        except requests.exceptions.ConnectionError:
            print(f"\néŒ¯èª¤ï¼šç„¡æ³•é€£ç·šè‡³æœ¬åœ° Ollama æœå‹™ ({self.api_url})ã€‚")
            exit()
        except Exception as e:
            print(f"å‘¼å«æœ¬åœ° Ollama API æˆ–è™•ç†å›å‚³æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def classify_from_content(self, text_content, url):
        """åŸ·è¡Œã€Œæ‘˜è¦ -> åˆ†é¡ã€çš„å…©éšæ®µæµç¨‹"""
        summary_prompt = get_summary_prompt(text_content[:8000])
        summary = self._call_ollama(summary_prompt, f"{url} [æ‘˜è¦éšæ®µ]")
        
        if not summary:
            print("éŒ¯èª¤ï¼šç¬¬ä¸€éšæ®µæœªèƒ½ç”¢ç”Ÿæ‘˜è¦ã€‚")
            return None
        
        print(f"INFO: ç¬¬ä¸€éšæ®µæ‘˜è¦å®Œæˆ: {summary}")

        classification_result = None
        for attempt in range(MAX_CLASSIFICATION_RETRIES):
            classification_prompt = get_classification_from_summary_prompt(self.schema_json_str, url, summary)
            result = self._call_ollama(classification_prompt, f"{url} [åˆ†é¡éšæ®µ]", expect_json=True)
            if result and result.get("main_category_code"):
                classification_result = result
                break
            print(f"è­¦å‘Šï¼šåˆ†é¡éšæ®µå¤±æ•—ã€‚å°‡åœ¨ {RETRY_DELAY} ç§’å¾Œé€²è¡Œç¬¬ {attempt + 2} æ¬¡é‡è©¦...")
            time.sleep(RETRY_DELAY)
        
        if not classification_result:
            print("éŒ¯èª¤ï¼šç¬¬äºŒéšæ®µé‡è©¦å¤šæ¬¡å¾Œä»æœªèƒ½ç”¢ç”Ÿåˆ†é¡ã€‚")
            return None
            
        classification_result["summary"] = summary
        return classification_result


class WebScraper:
    """è² è²¬æŠ“å–ç¶²é å…§å®¹ï¼Œå…·å‚™ Selenium å‚™æ´æ©Ÿåˆ¶"""
    def __init__(self):
        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    def fetch(self, url):
        """ä¸»æŠ“å–å‡½å¼ï¼Œå„ªå…ˆä½¿ç”¨ requests"""
        print(f"æ­£åœ¨ä½¿ç”¨ Requests å˜—è©¦æŠ“å– {url} ...")
        try:
            response = requests.get(url, headers=self.headers, timeout=15, allow_redirects=True)
            if response.status_code == 200:
                print("Requests æŠ“å–æˆåŠŸã€‚")
                return response.content, response.url
            else:
                print(f"Requests å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}ã€‚å°‡å˜—è©¦ä½¿ç”¨ Seleniumã€‚")
                return self._fetch_with_selenium(url)
        except requests.exceptions.RequestException as e:
            print(f"Requests ç™¼ç”ŸéŒ¯èª¤: {e}ã€‚å°‡å˜—è©¦ä½¿ç”¨ Seleniumã€‚")
            return self._fetch_with_selenium(url)

    def _fetch_with_selenium(self, url):
        """ä½¿ç”¨ Selenium ä½œç‚ºå‚™æ´æŠ“å–æ–¹å¼"""
        if not SELENIUM_AVAILABLE:
            print("è­¦å‘Š: æœªå®‰è£ Seleniumï¼Œç„¡æ³•ä½¿ç”¨å‚™æ´æŠ“å–ã€‚")
            return None, None
        
        print(f"æ­£åœ¨ä½¿ç”¨ Selenium å˜—è©¦æŠ“å– {url} ...")
        options = ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        driver = None
        try:
            service = ChromeService(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            driver.get(url)
            time.sleep(5)
            page_source = driver.page_source
            final_url = driver.current_url
            print("Selenium æŠ“å–æˆåŠŸã€‚")
            return page_source, final_url
        except Exception as e:
            print(f"Selenium æŠ“å–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None, None
        finally:
            if driver:
                driver.quit()

class WebCrawler:
    """ä¸»çˆ¬èŸ²ç¨‹å¼ï¼Œæ¡ç”¨å…©éšæ®µåˆ†é¡ç­–ç•¥èˆ‡å‚™æ´æŠ“å–"""
    def __init__(self, start_urls, db_manager, classifier, scraper):
        self.db_manager = db_manager
        self.classifier = classifier
        self.scraper = scraper
        self.crawled_count = 0
        
        queue_from_db = self.db_manager.load_queue()
        if queue_from_db:
            self.urls_to_crawl = deque(queue_from_db)
            print(f"æˆåŠŸå¾è³‡æ–™åº«è¼‰å…¥ {len(queue_from_db)} å€‹å¾…è¾¦é …ç›®ã€‚")
        else:
            initial_root_urls = sorted(list(set(filter(None, [self.get_root_url(url) for url in start_urls]))))
            self.urls_to_crawl = deque(initial_root_urls)
            print("è³‡æ–™åº«ä¸­ç„¡å¾…è¾¦é …ç›®ï¼Œå¾ START_URLS åˆå§‹åŒ–ä½‡åˆ—ã€‚")
            for url in initial_root_urls:
                self.db_manager.add_to_queue(url)
        
        self.processed_domains = {self.get_domain(url) for url in self.urls_to_crawl}
        print(f"DEBUG: åˆå§‹åŒ–å®Œæˆï¼Œå·²è™•ç†/å¾…è™•ç†åŸŸåå…± {len(self.processed_domains)} å€‹ã€‚")


    def get_domain(self, url):
        try: return urlparse(url).netloc
        except: return None

    def get_root_url(self, url):
        try:
            parsed = urlparse(url)
            return f"{parsed.scheme}://{parsed.netloc}" if parsed.scheme and parsed.netloc else None
        except: return None

    def _save_classification(self, domain, url, classification_result):
        """å°‡åˆ†é¡çµæœå„²å­˜è‡³è³‡æ–™åº«çš„è¼”åŠ©å‡½å¼"""
        main_cat_code = classification_result.get("main_category_code")
        sub_cat_code = classification_result.get("subcategory_code")
        
        if main_cat_code not in MAIN_CATEGORY_MAP or sub_cat_code not in SUBCATEGORY_MAP or not sub_cat_code.startswith(main_cat_code):
             print(f"è­¦å‘Š: AI å›å‚³äº†ç„¡æ•ˆæˆ–ä¸åŒ¹é…çš„ä»£ç¢¼ã€‚Main: {main_cat_code}, Sub: {sub_cat_code}")
             return False
        
        summary = classification_result.get("summary", "ç„¡æ³•ç”Ÿæˆæ‘˜è¦")
        main_cat_name = MAIN_CATEGORY_MAP.get(main_cat_code, "æœªçŸ¥")
        sub_cat_name = SUBCATEGORY_MAP.get(sub_cat_code, "æœªçŸ¥")
        
        self.db_manager.add_domain_classification(domain, main_cat_code, main_cat_name, sub_cat_code, sub_cat_name, summary, url)
        self.crawled_count += 1
        return True

    def _find_and_queue_new_links(self, soup, base_url):
        """å¾é é¢ä¸­å°‹æ‰¾æ–°çš„ã€æœªè™•ç†éçš„æ ¹ URL ä¸¦åŠ å…¥ä½‡åˆ—"""
        for link in soup.find_all('a', href=True):
            href = link['href']
            if not href: continue
            
            absolute_url = urljoin(base_url, href)
            new_domain = self.get_domain(absolute_url)
            
            if new_domain and new_domain not in self.processed_domains:
                root_url = self.get_root_url(absolute_url)
                if root_url:
                    self.processed_domains.add(new_domain)
                    self.urls_to_crawl.append(root_url)
                    self.db_manager.add_to_queue(root_url) 
                    print(f"ç™¼ç¾æ–°åŸŸå {new_domain}ï¼Œå·²å°‡æ ¹ URL åŠ å…¥ä½‡åˆ—: {root_url}")

    def run(self, max_domains):
        """åŸ·è¡Œçˆ¬èŸ²ä¸»è¿´åœˆ"""
        while self.urls_to_crawl and self.crawled_count < max_domains:
            url = self.urls_to_crawl.popleft()
            self.db_manager.remove_from_queue(url)
            
            domain = self.get_domain(url)

            if not domain or self.db_manager.domain_exists(domain):
                continue
            
            print(f"\n--- é–‹å§‹è™•ç† ({self.crawled_count + 1}/{max_domains}): {url} ---")
            
            html_content, final_url = self.scraper.fetch(url)

            if not html_content:
                print(f"éŒ¯èª¤: ä½¿ç”¨æ‰€æœ‰æ–¹æ³•æŠ“å– {url} çš†å¤±æ•—ã€‚å°‡æ­¤åŸŸåæ¨™è¨˜ç‚ºéŒ¯èª¤ã€‚")
                self._save_classification(domain, url, {"main_category_code": "999", "subcategory_code": "999-02", "summary": "çˆ¬èŸ²ç„¡æ³•è¨ªå•æ­¤ç¶²ç«™ã€‚"})
                continue

            soup = BeautifulSoup(html_content, 'html.parser')
            
            print("INFO: é–‹å§‹é€²è¡Œç¶²é å…§å®¹åˆ†æ...")
            for tag in soup(["script", "style", "header", "footer", "nav", "aside"]):
                tag.decompose()
            text_content = soup.get_text(separator=' ', strip=True)

            if text_content and len(text_content) > 150:
                content_result = self.classifier.classify_from_content(text_content, final_url)
                if content_result:
                    self._save_classification(self.get_domain(final_url), final_url, content_result)
                else:
                    print(f"åŸŸå {domain} çš„å…§å®¹åˆ†æå¤±æ•—ã€‚")
            else:
                print(f"åŸŸå {domain} çš„æ–‡å­—å…§å®¹å¤ªå°‘ï¼Œç„¡æ³•é€²è¡Œå…§å®¹åˆ†æã€‚")

            self._find_and_queue_new_links(soup, final_url)
            
            time.sleep(1)

        print(f"\nçˆ¬å–å®Œæˆï¼ç¸½å…±è™•ç†äº† {self.crawled_count} å€‹åŸŸåã€‚")

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    if not SELENIUM_AVAILABLE:
        print("è­¦å‘Šï¼šæœªå®‰è£ Selenium ç›¸é—œå¥—ä»¶ï¼Œå‚™æ´æŠ“å–åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")
        print("å»ºè­°åŸ·è¡Œï¼špip install selenium webdriver-manager")

    if not USE_LOCAL_AI:
        print("éŒ¯èª¤ï¼šç›®å‰çš„å…©éšæ®µåˆ†é¡é‚è¼¯å°šæœªç‚º Gemini API é€²è¡Œå„ªåŒ–ã€‚è«‹å°‡ USE_LOCAL_AI è¨­ç‚º Trueã€‚")
        return
        
    classifier = LocalOllamaClassifier(model=LOCAL_AI_MODEL, api_url=LOCAL_AI_URL, schema_json=CLASSIFICATION_SCHEMA_JSON)

    db_manager = None
    scraper = WebScraper()
    try:
        db_manager = DatabaseManager(DB_NAME)
        db_manager.setup_tables() 
        crawler = WebCrawler(start_urls=START_URLS, db_manager=db_manager, classifier=classifier, scraper=scraper)
        crawler.run(max_domains=MAX_DOMAINS_TO_CRAWL)
    except Exception as e:
        print(f"ç¨‹å¼åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
    finally:
        if db_manager:
            db_manager.close()

if __name__ == "__main__":
    main()
